# Related Work: Out-of-Distribution Detection and Generalization (2023–2025)

## Overview and Taxonomy
Recent work on out-of-distribution (OOD) detection and OOD generalization consolidated into several methodological families: training-free and post-hoc scoring on pre-trained models; energy/logit methods and their calibrated training variants; representation- and prototype-based scoring; generative and diffusion-based approaches for detection, augmentation, or test-time adaptation; prompt- and text–image alignment methods for vision–language models (VLMs); and online/continual adaptation for streaming shifts. Across these, evaluation concerns have shifted toward disentangling semantic shift from covariate shift at ImageNet scale, principled OOD decision rules with guarantees, and extensions from image classification to dense prediction and detection, with emerging multimodal settings. Large-scale pretraining (ViTs, CLIP, diffusion) materially improves separability, but naïve scaling or query expansion does not solve OOD on its own and can even harm open-set performance without careful design and evaluation protocols [5][6]. Benchmarks typically report AUROC, AUPR, and FPR@95 for OOD discrimination, and calibration metrics such as ECE/NLL when selective prediction or open-set recognition is emphasized; ImageNet-scale re-benchmarks stress covariate-vs-semantic shift separation and realistic acquisition artifacts [5][35].

## Post-hoc scoring on pre-trained encoders: logits, energies, and features
A strong thread retains post-hoc scoring that modifies or leverages representations of fixed encoders. Discriminability-Driven Channel Selection identifies feature channels that most cleanly separate in-distribution (ID) classes and prunes/rectifies them to improve OOD separation across backbones and scoring rules, yielding consistent AUROC/FPR@95 gains with minimal overhead on CIFAR/ImageNet [3]. Orthogonal projections of gradients remove ID-important subspaces before scoring (GradOrth), reducing FPR@95 versus MSP and energy baselines efficiently [22]. Gradient-based attribution abnormality (GAIA) uses explanation saliency distributions as OOD signals, offering large FPR@95 reductions with only gradient computation overhead on CIFAR/ImageNet [21]. Representation-smoothness across layers is turned into a score: stable between-layer transformations for ID become unstable on OOD, improving AUROC and FPR@95 post-hoc across architectures [19]; neuron activation coverage similarly correlates with generalization and provides a score usable across datasets [20]. Earlier CVPR’23 work strengthened logit/energy families by decoupling max-logit into cosine-similarity versus norm components for more stable thresholding (DML/DML+) [25], and by Shapley-style important-neuron selection plus activation clipping for robust post-hoc scoring (LINe) [24]. Collectively, these methods require no retraining, modest compute, and mostly tune few hyperparameters (temperature, channel thresholds), making them attractive baselines at scale [3][21][22][19][20][24][25].

## Energy-based training and Outlier Exposure (OE), imbalance, and boundary sharpening
Training-based methods continue to show strong absolute detection when auxiliary outliers are available. Energy-based “Hopfield boosting” replaces standard loss with a modern Hopfield energy to emphasize boundary outliers during OE, driving down FPR@95 on CIFAR-10/100 and ImageNet-1k to new levels, albeit with the cost of OE data curation and training [17]. Diversified OE (DivOE) synthesizes informative outliers by extrapolating from seeds, improving over vanilla OE across CIFAR/ImageNet without massive external datasets [23]. Balanced energy regularization addresses class imbalance during OE and shows gains for both image classification and semantic segmentation OOD [26]. A complementary line corrects the bias of OOD scores induced by long-tailed ID (NeurIPS’24 “Imbalanced ID”), delivering consistent improvements across CIFAR/IMAGENET long-tail variants via statistical regularization [18]. These works converge on the view that OE remains the most potent lever for detection error and FPR@95—when auxiliary data are carefully constructed or synthesized—but incurs greater sensitivity to OE distribution coverage and training hyperparameters [17][23][26][18].

## Vision–Language models (CLIP/VLMs): prompts, negative semantics, and semantic pools
With CLIP-scale pretraining, VLM-based OOD methods emerged as a dominant post-hoc/few-shot regime. Learned negative prompts transfer across datasets and labels to separate ID from OOD for zero-shot/few-shot classification with light prompt tuning (NegPrompt) [1]. Zero-shot prompt augmentation (TAG) diversifies textual descriptions to widen ID–OOD margins without training, improving AUROC/FPR@95 on CIFAR/ImageNet [7]. Expanding the text concept universe via low-dependence “conjugated semantic pools” improves separability over naïve query scaling (which can saturate or degrade), and can be used post-hoc or with light tuning [8]. Training-free weight-space modification via selective low-rank approximation (SeTAR) reshapes decision geometry for better OOD without retraining; a fine-tuned variant further improves performance [9]. Self-calibrated tuning (SCT) corrects for background spurious cues, improving calibration and OOD in few-shot settings [10]. Earlier, CLIPN trained CLIP to say “No” with a dedicated text-side “rejection” concept to enable threshold-free variants [11], and LoCoOp regularized prompts with local visual features to suppress nuisance correlations in few-shot OOD [12]. Importantly, analyses of open-set recognition in the VLM era caution that closed-world query sets and naïve scaling can entangle OD/OOD semantics; robust evaluation requires protocols and metrics tailored to open-set retrieval/recognition with VLMs [6]. Overall, VLM-based methods are mostly post-hoc or lightly tuned, benefit from scaling, and are relatively compute-efficient; their main sensitivities are prompt design, negative semantics coverage, and the alignment between ID labels and the broader semantic pool [1][7][8][9][10][11][12][6].

## Generative and diffusion-based methods: detection, augmentation, and test-time adaptation
Generative modeling resurged via diffusion both as a detector and as a tool to improve robustness. A single unconditional diffusion model can be repurposed to detect OOD by analyzing curvature and rate-of-change along reverse trajectories, delivering competitive detection across multiple tasks without task-specific training (DiffPath) [2]. For test-time adaptation (TTA), diffusion-based guidance refines inputs or features under various shifts (style, corruption, sketch), boosting accuracy and robustness without updating ID model weights (GDA) [13]. As a training-time tool, diffuse-and-denoise augmentation performs single-step reverse diffusion to generate realistic variants, stacking with AugMix/DeepAugment to improve both corruption and OOD detection robustness on ResNets and ViTs with low overhead (DiffAug) [14]. In dense prediction, synthetic data via diffusion or Stable Diffusion fine-tuning probes segmentation reliability and OOD segmentation robustness without changing task model capacity, offering a pragmatic stress test for distribution shift [15]. These approaches leverage foundation-model scaling and are sensitive to diffusion schedule/guidance hyperparameters; they reduce reliance on external OE but add inference/training-time cost [2][13][14][15].

## Dense prediction and detection: pixel-level and object-level OOD
OOD detection extended beyond image classification to pixel- and object-level tasks. Segment Every OOD Object proposes a training framework to discover and segment OOD instances, reporting pixel-/region-level AUROC and FPR@95 on standard segmentation anomaly benchmarks [28]. Training-free, dataset-agnostic PixOOD builds online condensed representations of intra-class pixel variability and obtains state-of-the-art performance on four of seven segmentation datasets with strong generality [29]. For object detection, SAFE extracts sensitivity-aware features (e.g., from BN residuals) and trains a small MLP using an adversarial-vs-clean proxy task to improve OOD detection without OOD labels, reducing FPR@95 markedly on OpenImages [30]. These task-specific lines emphasize robust OOD under complex outputs and highlight the need for pixel- or instance-level metrics and protocols distinct from classification [28][29][30].

## Test-time, online, and continual OOD
Streaming deployment motivates dynamic, threshold-light strategies. OODD maintains a dynamic dictionary of evolving OOD features for test-time OOD detection; it is training-free at deployment, improves FPR@95 on OpenOOD splits, and includes efficient KNN variants for speed [4]. In continual and task-incremental scenarios, hierarchical two-sample tests (H2ST) provide a threshold-free hypothesis-testing framework over feature maps, identifying tasks and OOD jointly and outperforming replay-based TIL methods [31]. For multimodal streams (video, flow, audio), Dynamic Prototype Updating (DPU) acts as a plug-and-play enhancement for nine base OOD algorithms, yielding large far-OOD gains across multiple datasets by updating memory/prototypes online [32]. These methods move toward operational deployment with bounded memory/compute overhead, though they require careful choices of memory size and update schedules to remain stable [4][31][32].

## Benchmarks, analyses, and principled decision rules
ImageNet-OOD introduced a re-benchmark that cleans semantic shift evaluation and shows that many detectors are overly sensitive to covariate shift, calling for stronger definitions and protocols at scale [5]. ImageNet-ES further highlights that real capture variations (environment/sensor) can break detectors, and that sensor-aware controls help diagnose robustness gaps [35]. On the decision-rule side, multiple testing and conformal-style approaches deliver finite-sample guarantees: JMLR’23 formalizes a principled multiple-testing approach to combine OOD statistics with bounded false discoveries [33], and ICML’24 proposes a generalized Benjamini–Hochberg (g-BH) rule with FDR control and vanishing FPR bounds under assumptions, in a model-agnostic, post-hoc fashion [16]. Foundational theory on the learnability of OOD detection establishes necessary conditions and impossibility results under natural data models, helping interpret when score-based detection can generalize [34]. These works complement score design by supplying operational criteria for thresholding and deployment [5][35][33][16][34].

## Cross-modal perspectives: VLMs and multimodal extensions
Cross-modal work is led by VLM-based OOD in vision–language, where prompt calibration, negative semantics, and semantic-pool design improve zero-/few-shot detection on CLIP backbones while remaining training-free or lightly tuned [1][7][8][9][10][11][12]. Analyses warn against naïve query scaling in open-set recognition with VLMs, urging protocol design that faithfully reflects open-world operation [6]. Multimodal OOD beyond images (e.g., video/audio) is addressed by prototype-based updates and plug-in modules that enhance base algorithms under stream shifts [32]. Principled decision rules based on multiple testing are model- and modality-agnostic, offering a path to unify thresholding across text, vision, and multimodal detectors [33][16]. While core ideas—representation separation, calibrated abstention, semantic coverage, and principled thresholding—transfer across modalities, standardized, top-venue benchmarks in language for OOD detection remain less consolidated than in vision, motivating multimodal and modality-agnostic evaluation protocols as they mature [6][33][16].

## Emergent trends, empirical setups, and open problems
- Training-free vs. training-based: Post-hoc representation and prompt/VLM methods achieve strong efficiency–accuracy trade-offs across CIFAR/ImageNet; OE-based training still delivers the strongest absolute FPR@95 when feasible but demands curated/synthesized outliers and careful tuning [3][1][7][9][17][23][26].  
- Role of pretraining and scaling: ViT/CLIP/diffusion pretraining consistently improves separability; however, scaling query sets without principled design can undermine open-set performance, and detectors remain vulnerable to covariate and sensor shifts [6][5][2][13][14].  
- Shift types and tasks: Most work targets semantic open-set classification; segmentation and object detection OOD grows, with pixel-/instance-level metrics; test-time and continual OOD methods begin to address streaming/open-world settings [28][29][30][4][31].  
- Benchmarks and metrics: AUROC, AUPR, and FPR@95 dominate; calibration (ECE/NLL) appears when abstention/selective prediction is studied; ImageNet-OOD and ImageNet-ES strengthen evaluation by clarifying semantic vs covariate/sensor effects [5][35].  
- Practical constraints: Training-free/post-hoc methods are light on compute and less sensitive to hyperparameters; diffusion-based TTA/augmentation incurs additional inference/training steps but generalizes across shift types; online methods must balance memory and stability [3][14][13][2][4][31].  
- Open problems: Thresholding under class imbalance and long-tail remains challenging; consistent, modality-agnostic decision rules with operational guarantees are promising; robust definitions and benchmarks for open-set VLMs and multimodal streams are needed; and disentangling semantic from covariate/sensor shift at scale continues to be a key evaluation gap [18][16][6][5][35][33][34].

### Sources
[1] Learning Transferable Negative Prompts for Out-of-Distribution Detection (CVPR 2024): https://openaccess.thecvf.com/content/CVPR2024/html/Li_Learning_Transferable_Negative_Prompts_for_Out-of-Distribution_Detection_CVPR_2024_paper.html  
[2] Out-of-Distribution Detection with a Single Unconditional Diffusion Model (NeurIPS 2024): https://proceedings.neurips.cc/paper_files/paper/2024/hash/4dc37a7bc61057252ce043fa3b83aac2-Abstract-Conference.html  
[3] Discriminability-Driven Channel Selection for Out-of-Distribution Detection (CVPR 2024): https://openaccess.thecvf.com/content/CVPR2024/html/Yuan_Discriminability-Driven_Channel_Selection_for_Out-of-Distribution_Detection_CVPR_2024_paper.html  
[4] OODD: Test-time Out-of-Distribution Detection with Dynamic Dictionary (CVPR 2025): https://openaccess.thecvf.com/content/CVPR2025/html/Yang_OODD_Test-time_Out-of-Distribution_Detection_with_Dynamic_Dictionary_CVPR_2025_paper.html  
[5] ImageNet-OOD: Deciphering Modern OOD Algorithms (ICLR 2024): https://openreview.net/forum?id=VTYg5ykEGS  
[6] Open-Set Recognition in the Age of Vision–Language Models (ECCV 2024): https://eccv.ecva.net/virtual/2024/poster/394  
[7] TAG: Text Prompt Augmentation for Zero-shot OOD Detection (ECCV 2024): https://eccv.ecva.net/virtual/2024/poster/772  
[8] Conjugated Semantic Pool Improves OOD with Pre-trained VLMs (NeurIPS 2024): https://proceedings.neurips.cc/paper_files/paper/2024/hash/967017dbe801dc95f5a2587c6d6a1ef3-Abstract-Conference.html  
[9] SeTAR: Selective Low-Rank Approximation for OOD (NeurIPS 2024): https://proceedings.neurips.cc/paper_files/paper/2024/hash/8555cf308efef02adacb0f9e09bd5f10-Abstract-Conference.html  
[10] Self-Calibrated Tuning for VLMs (NeurIPS 2024): https://proceedings.neurips.cc/paper_files/paper/2024/hash/666e5e1df2d04dbe2b545ea3a3e3f7d3-Abstract-Conference.html  
[11] CLIPN: Teaching CLIP to Say “No” (ICCV 2023): https://openaccess.thecvf.com/content/ICCV2023/html/Wang_CLIPN_for_Zero-Shot_OOD_Detection_Teaching_CLIP_to_Say_No_ICCV_2023_paper.html  
[12] LoCoOp: Few-shot OOD via Prompt Learning (NeurIPS 2023): https://proceedings.neurips.cc/paper_files/paper/2023/hash/f0606b882692637835e8ac981089eccd-Abstract-Conference.html  
[13] GDA: Generalized Diffusion for Robust Test-time Adaptation (CVPR 2024): https://openaccess.thecvf.com/content/CVPR2024/html/Tsai_GDA_Generalized_Diffusion_for_Robust_Test-time_Adaptation_CVPR_2024_paper.html  
[14] DiffAug: Diffuse-and-Denoise Augmentation (NeurIPS 2024): https://proceedings.neurips.cc/paper_files/paper/2024/hash/24e8b46430df965674221665816a4964-Abstract-Conference.html  
[15] Reliability in Semantic Segmentation: Can We Use Synthetic Data? (ECCV 2024): https://link.springer.com/chapter/10.1007/978-3-031-73337-6_25  
[16] A Provable Decision Rule for Out-of-Distribution Detection (ICML 2024): https://proceedings.mlr.press/v235/ma24t.html  
[17] Energy-based Hopfield Boosting for OOD (NeurIPS 2024): https://proceedings.neurips.cc/paper_files/paper/2024/hash/ee20461718669c6c9c5da478d46d60d9-Abstract-Conference.html  
[18] Rethinking OOD Detection under Imbalanced In-Distribution (NeurIPS 2024): https://proceedings.neurips.cc/paper_files/paper/2024/hash/c554c1305b8f4f993db4738a9c633d14-Abstract-Conference.html  
[19] OOD by Leveraging Between-Layer Transformation Smoothness (ICLR 2024): https://openreview.net/forum?id=ZoAWzxsi3c  
[20] Neuron Activation Coverage for OOD Detection and Generalization (ICLR 2024): https://openreview.net/forum?id=9ffYZ2B5co  
[21] GAIA: Gradient-based Attribution Abnormality for OOD (NeurIPS 2023): https://proceedings.neurips.cc/paper_files/paper/2023/hash/fcdccd419c4dc471fa3b73ec97b53789-Abstract-Conference.html  
[22] GradOrth: Orthogonal Projection for OOD Detection (NeurIPS 2023): https://proceedings.neurips.cc/paper_files/paper/2023/hash/77cf940349218069bbc230fc2c9c8a21-Abstract-Conference.html  
[23] DivOE: Diversified Outlier Exposure (NeurIPS 2023): https://proceedings.neurips.cc/paper_files/paper/2023/hash/46d943bc6a15a57c923829efc0db7c7a-Abstract-Conference.html  
[24] LINe: Leveraging Important Neurons (CVPR 2023): https://cvpr2023.thecvf.com/virtual/2023/poster/22068  
[25] Decoupling MaxLogit for OOD Detection (CVPR 2023): https://cvpr2023.thecvf.com/virtual/2023/poster/21982  
[26] Balanced Energy Regularization Loss for OOD (CVPR 2023): https://cvpr2023.thecvf.com/virtual/2023/poster/21978  
[27] Uncertainty-Aware Optimal Transport for Semantically Coherent OOD (CVPR 2023): https://cvpr.thecvf.com/virtual/2023/poster/23035  
[28] Segment Every Out-of-Distribution Object (CVPR 2024): https://openaccess.thecvf.com/content/CVPR2024/html/Zhao_Segment_Every_Out-of-Distribution_Object_CVPR_2024_paper.html  
[29] PixOOD: Pixel-Level OOD Detection (ECCV 2024): https://eccv.ecva.net/virtual/2024/poster/828  
[30] SAFE: Sensitivity-Aware Features for OOD Object Detection (ICCV 2023): https://openaccess.thecvf.com/content/ICCV2023/html/Wilson_SAFE_Sensitivity-Aware_Features_for_Out-of-Distribution_Object_Detection_ICCV_2023_paper.html  
[31] H2ST: Hierarchical Two-Sample Tests for Continual OOD (CVPR 2025): https://cvpr.thecvf.com/virtual/2025/poster/33768  
[32] DPU: Dynamic Prototype Updating for Multimodal OOD (CVPR 2025): https://cvpr.thecvf.com/virtual/2025/poster/33698  
[33] A Principled Approach to OOD Detection via Multiple Testing (JMLR 2023): https://www.jmlr.org/papers/v24/23-0838.html  
[34] On the Learnability of Out-of-Distribution Detection (JMLR 2024): https://jmlr.org/beta/papers/v25/23-1257.html  
[35] ImageNet-ES: Environment and Sensor Shift Benchmark (CVPR 2024 program): https://cvpr.thecvf.com/virtual/2024/poster/30006  
[36] CADet: A Self-Supervised Approach for OOD Detection (NeurIPS 2023): https://proceedings.neurips.cc/paper_files/paper/2023/hash/1700ad4e6252e8f2955909f96367b34d-Abstract-Conference.html  
[37] Characterizing OOD Error via Optimal Transport (NeurIPS 2023): https://proceedings.neurips.cc/paper_files/paper/2023/hash/38fd51cf36f28566230a93a5fbeaabbf-Abstract-Conference.html