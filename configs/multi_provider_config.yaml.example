# Multi-Provider Evaluation Configuration
# Copy this to multi_provider_config.yaml and customize

# Unique name for this evaluation run (used for state tracking)
run_name: "my_evaluation_v1"

# Input JSON files to evaluate (from preprocessing step)
input_files:
  - extracted_reports/reports_20250101_120000.json
  # Add more JSON files here

# Evaluation criteria
criteria:
  - presentation
  - consistency
  - coverage
  - citation
  # Note: depth requires pairwise comparison setup

# Providers to use (will grade with each and then average)
providers:
  - openai
  - gemini

# Specific models for each provider (optional)
models:
  openai: gpt-5-2025-08-07
  gemini: gemini-2.5-pro

# Output directory for results
output_dir: results

# Directory for state tracking (for resume capability)
state_dir: evaluation_state

# Maximum concurrent API calls
max_concurrent: 5


