# LiveResearchBench

A comprehensive evaluation benchmark for AI-generated long-form research reports.

## Overview

LiveResearchBench evaluates research reports across **5 key criteria** using state-of-the-art LLM-as-a-judge methodologies:

- **â¶ Presentation & Organization** (Checklist-based, binary 0/1)
- **â· Factual & Logical Consistency** (Pointwise/additive, score 10-100)
- **â¸ Coverage & Comprehensiveness** (Checklist-based, binary 0/1)
- **â¹ Analysis Depth** (Pairwise comparison, 5 dimensions)
- **âº Citation Association** (Pointwise/additive, score 10-100)

Each criterion uses the most appropriate evaluation protocol based on human alignment studies, ensuring high-quality, reliable assessments.

## Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/your-org/LiveResearchBench.git
cd LiveResearchBench

# Create virtual environment and install dependencies with uv
uv venv
source .venv/bin/activate
uv sync

# Configure API keys
cp .env.example .env
# Edit .env and add your API keys:
#   OPENAI_API_KEY=your-key
#   GEMINI_API_KEY=your-key
```

### Basic usage to evaluate long-form reports

**1. Preprocess Reports** (Create a JSON index mapping queries to report file locations)

```bash
# Process all models in a directory (recommended: use --use-realtime for live benchmark queries)
python preprocess.py /path/to/model_outputs --use-realtime

# Or process specific models (subdirectories) only
python preprocess.py /path/to/model_outputs -m gpt-5-search gemini-pro --use-realtime

# With custom output directory
python preprocess.py /path/to/model_outputs -o extracted_reports/ --use-realtime

# Optional: Use static queries without placeholder replacement (not recommended for live evaluation)
python preprocess.py /path/to/model_outputs
```

**Expected input directory structure:**
```
/path/to/model_outputs/
â”œâ”€â”€ model_name_1/
â”‚   â”œâ”€â”€ qid_<qid>_report.md
â”‚   â”œâ”€â”€ qid_<qid>_report.md
â”‚   â””â”€â”€ ...
â”œâ”€â”€ model_name_2/
â”‚   â”œâ”€â”€ qid_<qid>_report.md
â”‚   â””â”€â”€ ...
```

**Expected output structure:**

After preprocessing, a JSON file will be created in `extracted_reports/` (or the specified output directory) with the naming pattern `reports_{timestamp}.json`.

The JSON structure includes:

```json
{
  "metadata": {
    "timestamp": "20250101_120000",              // When the preprocessing was performed
    "total_reports": 300,                         // Total number of reports contained and processed
    "total_models": 3,                           // Number of model outputs included
    "base_path": "/path/to/model_outputs",       // Base path to report outputs directory
    "use_realtime": true                        // Whether to replace query placeholders with real-time values
  },
  "reports": [
    {
      "model_name": "model-name-1",              // Model/system name (subdirectory name)
      "query_id": "abc123xyz",               // Query identifier from the benchmark
      "query": "Research query text...",         // Query loaded and processed from LiveResearchBench dataset
      "report_file_path": "/path/to/model_outputs/model-name-1/qid_abc123xyz_report.md"
    },
    {
      "model_name": "model-name-2",
      "query_id": "def456uvw",
      "query": "Another research query...",
      "report_file_path": "/path/to/model_outputs/model-name-2/qid_def456uvw_report.md"
    }
    // ... more report entries
  ]
}
```

Each report entry contains the model name, query ID, full query text (loaded and processed from the LiveResearchBench dataset on HuggingFace), and the absolute path to the markdown report file.

**2. Grade Single File**

```bash
# Single criterion
python main.py \
    --input extracted_reports/reports_20250101_120000.json \ # the json file created from preprocessing
    --criteria presentation \
    --provider gemini

# Multiple criteria
python main.py \
    --input extracted_reports/reports_20250101_120000.json \
    --criteria presentation,consistency,citation,coverage,depth \
    --provider openai --model gpt-5-2025-08-07
```

**3. Batch Grade All Models**

```bash
# Single provider
./scripts/batch_grade.sh my_experiment

# Both providers (GPT-5 + Gemini) with averaging
./scripts/batch_grade_multi_provider.sh my_experiment
```

## Evaluation Protocols

Based on our human alignment study, we adopt three different protocols:

### 1. Checklist-Based (Presentation, Coverage)
- **Binary scoring**: 0 (fail) or 1 (pass) for each checklist item
- **Presentation**: 10 fixed quality questions
- **Coverage**: Custom checklist per query

### 2. Pointwise/Additive (Consistency, Citation)
- **Error counting**: Identify and count specific issues
- **Score**: 10-100 based on number of issues found
- **Consistency**: Count logical/factual contradictions
- **Citation**: Count missing citations

### 3. Pairwise Comparison (Depth)
- **Side-by-side comparison**: Compare two reports directly
- **5 dimensions**: Granularity, Insight, Critique, Evidence, Density
- **Three judges**: Majority voting for reliability

## Directory Structure

```
LiveResearchBench/
â”œâ”€â”€ liveresearchbench/          # Main Python package
â”‚   â”œâ”€â”€ common/                 # Shared utilities
â”‚   â”œâ”€â”€ graders/                # Grading implementations
â”‚   â”œâ”€â”€ criteria/               # Criterion definitions
â”‚   â””â”€â”€ batch_evaluator.py      # Batch grading orchestrator
â”œâ”€â”€ preprocess.py               # Preprocessing script
â”œâ”€â”€ main.py                     # Main grading script
â”œâ”€â”€ average_results.py          # Multi-provider averaging
â”œâ”€â”€ scripts/                    # Bash convenience scripts
â”œâ”€â”€ configs/                    # Configuration files
â”œâ”€â”€ data/                       # Data files (checklists, etc.)
â””â”€â”€ tests/                      # Test suite
    â”œâ”€â”€ test_basic.py           # Unit tests
    â”œâ”€â”€ test_mock_grading.py    # Integration tests
    â””â”€â”€ run_all_tests.sh        # Test runner
```

## Configuration

### Batch Config (`configs/batch_config.yaml`)

```yaml
# List of JSON files to process
input_files:
  - /path/to/reports_model1.json
  - /path/to/reports_model2.json

# Criteria to evaluate
criteria:
  - presentation
  - consistency
  - coverage
  - citation
  - depth
  # Note: Questions and checklists are automatically loaded from HuggingFace
```

## Advanced Usage

### Multi-Provider Grading & Averaging

For more reliable results, grade with both GPT-5 and Gemini-2.5-Pro, then average:

```bash
# Method 1: Automated script
./scripts/batch_grade_multi_provider.sh my_experiment

# Method 2: Manual
python main.py --batch --config configs/batch_config.yaml \
    --provider openai --model gpt-5-2025-08-07 \
    --output-dir results/gpt5

python main.py --batch --config configs/batch_config.yaml \
    --provider gemini --model gemini-2.5-pro \
    --output-dir results/gemini

python average_results.py \
    --input-a results/gpt5/reports_graded.json \
    --input-b results/gemini/reports_graded.json \
    --output results/reports_averaged.json
```

### Resume Interrupted Runs

Grading automatically skips already-graded reports:

```bash
# Just run the same command again - it will resume
python main.py --batch --config configs/batch_config.yaml --provider gemini

# Or force re-grade everything
python main.py --batch --config configs/batch_config.yaml --provider gemini --force-regrade
```

### Filtering

Grade specific subsets of reports:

```bash
# Filter by experiment name (modify JSON before grading)
# Or process only specific JSON files in batch_config.yaml
```

## API Keys

Set in `.env` file:

```bash
# OpenAI (for GPT-5)
OPENAI_API_KEY=sk-...

# Google Gemini
GEMINI_API_KEY=AIza...
```

## Output Format

### Output Directory Structure

Grading results are organized by input file name, with timestamped result files inside:

```
results/
â””â”€â”€ reports_20251030_221703_graded_openai_gpt-5/
    â”œâ”€â”€ summary_2025-10-31T01-38-49.407295.json
    â””â”€â”€ detailed_results_2025-10-31T01-38-49.407295.json
```

Directory naming: `{input_json_stem}_graded_{provider}_{model}/`
File naming: `{filename}_{timestamp}.json` (ISO 8601 format)

After grading completes, a summary is automatically printed to the terminal:

```
================================================================================
ğŸ“Š GRADING SUMMARY
================================================================================

ğŸ” Provider: openai
ğŸ¤– Model: gpt-5-2025-08-07
ğŸ“… Graded at: 2025-10-31T01:38:49.407295
ğŸ“ Total reports: 2
ğŸ¯ Criteria: coverage

--------------------------------------------------------------------------------
ğŸ“ˆ OVERALL RESULTS (across all models)
--------------------------------------------------------------------------------

COVERAGE:
  Mean:  38.89
  Min:   0.00
  Max:   77.78
  Count: 2

--------------------------------------------------------------------------------
ğŸ† RESULTS BY MODEL
--------------------------------------------------------------------------------

ğŸ“¦ deerflow-multi-agent:
  coverage:
    Mean: 38.89 | Min: 0.00 | Max: 77.78 | Count: 2

================================================================================
ğŸ’¾ RESULTS SAVED TO:
================================================================================
ğŸ“ Directory: results/reports_20251030_221703_graded_openai_gpt-5-2025-08-07
   â”œâ”€â”€ summary_2025-10-31T01-38-49.407295.json
   â””â”€â”€ detailed_results_2025-10-31T01-38-49.407295.json
================================================================================
```

### Summary File (`summary.json`)

Contains aggregated statistics:

```json
{
  "metadata": {
    "provider": "openai",
    "model": "gpt-5-2025-08-07",
    "graded_at": "2025-10-31T01:38:49.407295",
    "total_reports": 10,
    "criteria_evaluated": ["presentation", "coverage", "consistency", "citation"]
  },
  "results_by_model": {
    "model-name-1": {
      "presentation": {
        "mean": 85.5,           // Average pass rate across all reports
        "count": 5,             // Number of reports graded
        "min": 70.0,
        "max": 100.0
      },
      "coverage": { ... },
      "consistency": { ... }
    },
    "model-name-2": { ... }
  },
  "overall_results": {
    "presentation": {
      "mean": 82.3,             // Average across all models/reports
      "count": 10,
      "min": 60.0,
      "max": 100.0
    },
    // ... other criteria
  }
}
```

### Detailed Results File (`detailed_results.json`)

Each report is augmented with grading results:

```json
{
  "reports": [
    {
      "query_id": "qid_123",
      "query": "What is...",
      "report_file_path": "/path/to/report.md",
      
      "presentation_grading_results": {
        "provider": "gemini",
        "model": "gemini-2.5-pro",
        "graded_at": "2025-...",
        "evaluations": {
          "p1": {"score": 1, "justification": "..."},
          "p2": {"score": 0, "justification": "..."}
        },
        "summary": {
          "total_criteria": 10,
          "passed_count": 8,
          "average_pass_rate": 80.0
        }
      },
      
      "consistency_grading_results": {
        "provider": "gemini",
        "model": "gemini-2.5-pro",
        "graded_at": "2025-...",
        "specific_issues": ["Issue 1...", "Issue 2..."],
        "total_issues": 2,
        "score": 90,
        "justification": "..."
      }
    }
  ]
}
```

## Citation

If you use LiveResearchBench in your research, please cite:

```bibtex
@article{liveresearchbench,
  title   = {LiveResearchBench: A Live Benchmark for User-Centric Deep Research in the Wild},
  author  = {Wang, Jiayu and Ming, Yifei and Dulepet, Riya and Chen, Qinglin and Xu, Austin and Ke, Zixuan and Sala, Frederic and Albarghouthi, Aws and Xiong, Caiming and Joty, Shafiq},
  journal = {arXiv preprint arXiv:2510.14240},
  year    = {2025},
  url     = {https://arxiv.org/abs/2510.14240}
}
```
